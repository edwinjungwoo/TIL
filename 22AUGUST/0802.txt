
CS224n Lecture 9 수강 및 노트 정리

- Self-attention
- Transformer *****
- Encoder
- Decoder
- Word embedding + Positional representation
- Multi-head attention
- Residual Connection, Layer Normalization
- Feed-forward

- Masked Multi-head attention
- Multi-head Cross-Attention

Attention: Q dot product K , V와 가중합
